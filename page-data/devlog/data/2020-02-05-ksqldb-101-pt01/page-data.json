{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/devlog/data/2020-02-05-ksqldb-101-pt01","result":{"data":{"markdownRemark":{"id":"2ab54674-65d1-576a-b9c1-3c425470c5a9","html":"<p>이 글은 <code class=\"language-text\">ksqlDB</code>(당시에는 <code class=\"language-text\">KSQL</code>이라는 명칭이었습니다)를 학습하기 위해 정리한 연재글입니다.</p>\n<p>이 글의 순서는 아래와 같습니다.</p>\n<h1 id=\"목차\" style=\"position:relative;\"><a href=\"#%EB%AA%A9%EC%B0%A8\" aria-label=\"목차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>목차</h1>\n<ol>\n<li>part 1. 배경지식</li>\n<li><a href=\"https://blog.s3ich4n.me/devlog/data/2020-02-17-ksqldb-101-pt01\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">part 2. Kafka Streams에 대해</a></li>\n<li><a href=\"https://blog.s3ich4n.me/devlog/data/2020-03-10-ksqldb-101-pt03\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">part 3. 실전 예시: 오픈소스를 통해 살펴보는 실시간 보안 이벤트 탐지 룰</a></li>\n</ol>\n<h1 id=\"1-ksqldb를-알기-위해-필요한-것들\" style=\"position:relative;\"><a href=\"#1-ksqldb%EB%A5%BC-%EC%95%8C%EA%B8%B0-%EC%9C%84%ED%95%B4-%ED%95%84%EC%9A%94%ED%95%9C-%EA%B2%83%EB%93%A4\" aria-label=\"1 ksqldb를 알기 위해 필요한 것들 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. ksqlDB를 알기 위해 필요한 것들</h1>\n<p>ksqlDB를 이해하고 효율적으로 사용하기 위해 아래 기능들에 대해 이해하고 넘어갈 필요가 있습니다.</p>\n<ul>\n<li>(필수) Apache Kafka에 대한 기본적인 이해</li>\n<li>(필수) Kafka Streams</li>\n<li>(선택…이지만 사실상 필수라고 생각합니다) Schema Registry</li>\n</ul>\n<h2 id=\"사전-학습-아파치-카프카의-api들에-대하여\" style=\"position:relative;\"><a href=\"#%EC%82%AC%EC%A0%84-%ED%95%99%EC%8A%B5-%EC%95%84%ED%8C%8C%EC%B9%98-%EC%B9%B4%ED%94%84%EC%B9%B4%EC%9D%98-api%EB%93%A4%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC\" aria-label=\"사전 학습 아파치 카프카의 api들에 대하여 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>사전 학습: 아파치 카프카의 API들에 대하여</h2>\n<p>아파치 카프카의 몇몇 API에 대한 설명을 살펴봅시다. <a href=\"https://kafka.apache.org/documentation/#api\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">이 링크</a>를 살펴본다면 ksqlDB를 살펴볼 때에도 도움이 될 것입니다.</p>\n<ul>\n<li>Producer API: 특정 앱이 레코드 스트림을 발행하고 하나 이상의 카프카 토픽에 보내는 기능</li>\n<li>Consumer API: 특정 앱이 하나 이상의 토픽을 subscribe 하고 produce 받은 레코드의 스트림을 가공한다</li>\n<li>Streams API(Kafka Streams): 특정 앱이 stream processor로 작동하게 한다. 이는 하나 이상의 토픽으로부터 오는 input stream을 consume 하고, 하나 이상의 토픽에 produce하는 output stream을 produce한다. input stream을 output stream으로 바꾸는데 효과적이다.</li>\n<li>Connect API(Kafka Connects): 카프카 토픽을 다른 앱이나 데이터 시스템에 연결하는, 재사용 가능한 producer/consumer를 빌드/실행한다. 예시) RDBMS의 커넥터는 테이블의 모든 변화를 감지할 수 있다.</li>\n</ul>\n<p>너무 직역이라, 이를 풀어보자면 아래와 같습니다.</p>\n<ol>\n<li>아파치 카프카의 프로듀서 API, 컨슈머 API는 말 그대로, 카프카 토픽을 프로듀스/컨슘 하는데 사용되는 네이티브 API 입니다.</li>\n<li>스트림 API는, 한 토픽에서 다른 토픽으로 데이터의 흐름(streams of data)을 흘려보내는 기능을 의미합니다.</li>\n<li>커넥트 API는 외부 시스템에서 카프카로 데이터를 뽑아오거나, 카프카에서 외부 시스템으로 데이터를 저장하는(sink data system이라고 일컫습니다) 기능을 의미합니다.</li>\n</ol>\n<p>미리 말씀드리자면, ksqlDB는 이 스트림 API를 SQL문 작성 만으로 사용할 수 있도록 하는 애플리케이션이라 볼 수 있습니다.</p>\n<h2 id=\"schema-registry\" style=\"position:relative;\"><a href=\"#schema-registry\" aria-label=\"schema registry permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Schema Registry</h2>\n<ul>\n<li>(이하 스키마 레지스트리라고 부르겠습니다)</li>\n<li>상기 내용에서 보았듯, 데이터를 단순히 프로듀스/컨슘 하는 것 뿐 아니라 데이터의 흐름을 바꾸어서 다른곳에 저장하고, 또 불러오기도 합니다. 실수가 많을 수 있는 부분들에 대해 일종의 “객체화”를 수행하여, 이 데이터를 주고받도록 하면 실수가 줄겠죠.</li>\n<li>카프카의 스키마 관리 도구입니다.</li>\n<li>변조되거나 망가진 이벤트가 쓰이지 않도록 방지해줍니다.</li>\n<li>카프카로부터 Produce/Consume 할 데이터를 정의해둔 값</li>\n<li><a href=\"https://kafka.apache.org/documentation/#multitenancy-more\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">카프카 운영 시 웬만해선 사용하기를 권장합니다!</a></li>\n<li>스키마를 따로 관리하는 서버입니다.</li>\n<li>내부적으로 Avro, JSON 스키마, Protobuf 스키마를 사용할 수 있고 REST API로 스키마를 저장/조회할 수 있습니다.</li>\n</ul>\n<h3 id=\"avro란\" style=\"position:relative;\"><a href=\"#avro%EB%9E%80\" aria-label=\"avro란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Avro란?</h3>\n<ul>\n<li>아파치에서 작성한 데이터 직렬화 시스템입니다.</li>\n<li>Avro는 Schema에 의존적입니다. 쉽게 말해, 어떤식으로 직렬화/역직렬화 될지에 대한 스펙이라고 할 수 있지요.</li>\n<li>Avro 스키마는 JSON으로 정의되며, JSON 라이브러리를 사용하여 작성합니다.</li>\n</ul>\n<p>Avro는 스키마 정의를 아래의 방식으로 수행합니다.\n• 파일타입\n• 레코드의 길이\n• 레코드의 이름\n• 레코드 필드\n• 필요한 데이터타입에 맞게 정의</p>\n<p>아래와 같은 형식으로 담깁니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span><span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"typeName\"</span> ...attributes...<span class=\"token punctuation\">}</span></code></pre></div>\n<ul>\n<li>type\n<ul>\n<li>Primitive data type일 수도 있습니다.</li>\n<li>Complex type일 수도 있으며, 이 때는 <code class=\"language-text\">record</code> 라는 값을 별도로 기재합니다.</li>\n</ul>\n</li>\n<li>namespace\n<ul>\n<li>이 필드는 오브젝트가 위치한 네임스페이스를 정의합니다.</li>\n</ul>\n</li>\n<li>name\n<ul>\n<li>레코드의 이름을 가리키며, Avro API가 해당 데이터 타입을 참조할 때 쓰는 값입니다.</li>\n<li>schema name은 namespace와 함께 쓰입니다. 아래 예시를 보며 이해해봅시다.</li>\n</ul>\n</li>\n</ul>\n<p>e.g. document을 정의하고 namespace는 “Test”이며 이름은 “Employee”이고 2개의 필드를 가지고있다. 각각 string 타입의 “Name”, int 타입의 “Age”이다. <br /> 라고 하는 데이터 타입에 대한 Avro 정의는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"record\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"namespace\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Test\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Employee\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"fields\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Name\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"string\"</span> <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">{</span> <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">,</span> <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"int\"</span> <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<ul>\n<li>스키마 이름은, 위의 예시대로 쓰자면 <code class=\"language-text\">Test.Employee</code>가 되겠습니다.</li>\n</ul>\n<p>Avro의 primitive data type은 아래와 같습니다.</p>\n<table>\n<thead>\n<tr>\n<th>데이터타입</th>\n<th>상세</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code class=\"language-text\">null</code></td>\n<td>아무값도 없는 값</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">int</code></td>\n<td>32bit signed integer</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">long</code></td>\n<td>64bit signed integer</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">float</code></td>\n<td>IEEE 754를 따르는 32-bit 부동소수점 값</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">double</code></td>\n<td>IEEE 754를 따르는 64-bit 부동소수점 값</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">bytes</code></td>\n<td>8-bit unsigned bytes의 시퀀스</td>\n</tr>\n<tr>\n<td><code class=\"language-text\">string</code></td>\n<td>유니코드 캐릭터 시퀀스</td>\n</tr>\n</tbody>\n</table>\n<p>Avro의 Complex data type은 다음과 같으며, 이는 <code class=\"language-text\">\"type\": \"&lt;complex_data_type>\"</code> 와 같은 형태로 정의하며, 여기에서는 <em>일부만 소개</em> 하겠습니다.</p>\n<table>\n<thead>\n<tr>\n<th>데이터타입</th>\n<th>상세</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Record</td>\n<td>다양한 애트리뷰트들의 컬렉션</td>\n</tr>\n<tr>\n<td>Enum</td>\n<td>아이템 리스트에 대한 enumuration</td>\n</tr>\n<tr>\n<td>Arrays</td>\n<td>배열 형식의 값을 담을 수 있는 데이터 타입</td>\n</tr>\n<tr>\n<td>Maps</td>\n<td>key, value 쌍의 값을 담을 수 있는 데이터 타입</td>\n</tr>\n</tbody>\n</table>\n<p>그 외의 내용은 아래 링크를 통해 확인해보시기 바랍니다.</p>\n<ul>\n<li><a href=\"https://avro.apache.org/docs/current/spec.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://avro.apache.org/docs/current/spec.html</a></li>\n<li><a href=\"https://www.tutorialspoint.com/avro/avro_schemas.htm\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.tutorialspoint.com/avro/avro_schemas.htm</a></li>\n</ul>\n<h3 id=\"avro-serializer-를-어떻게-사용하나요\" style=\"position:relative;\"><a href=\"#avro-serializer-%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%82%98%EC%9A%94\" aria-label=\"avro serializer 를 어떻게 사용하나요 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Avro Serializer 를 어떻게 사용하나요?</h3>\n<p>자, 원하는 타입대로 직렬화/역직렬화를 수행할 수 있도록 포맷을 정했습니다. 그 다음은 어떤 절차를 거쳐야할까요? 아래 내용을 살펴봅시다. 이 내용은 파이썬의 <a href=\"https://github.com/confluentinc/confluent-kafka-python\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">confluent-kafka-python</a> 라이브러리를 통해 사용하였음을 말씀드립니다.</p>\n<ol>\n<li>프로듀서가 유효한 스키마를 가지고있는지 schema registry에 쿼리한다.</li>\n<li>schema registry는 스키마의 유효성에 따라 다르게 처리한다:\n<ol>\n<li>유효하지 않다면 <code class=\"language-text\">KafkaAvroSerializer</code>를 raise한다.</li>\n<li>유효하면 스키마 ID를 메시지에 추가하고 카프카에 쏜다.</li>\n</ol>\n</li>\n<li>필요에 따라 produce/consume을 한다(이 내용은 상세히 봅시다!)</li>\n</ol>\n<h4 id=\"produce-과정\" style=\"position:relative;\"><a href=\"#produce-%EA%B3%BC%EC%A0%95\" aria-label=\"produce 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Produce 과정</h4>\n<p><a href=\"https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/avro_producer.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">이 예시코드</a>를 살펴보시면 이해가 빠릅니다!</p>\n<ol>\n<li><code class=\"language-text\">AvroSerializer</code> 객체를 통해 Schema Registry 정보를 받고 직렬화/역직렬화 처리준비를 완료함</li>\n<li>카프카 정보를 받은 후 토픽에 <code class=\"language-text\">produce</code> 수행</li>\n</ol>\n<h4 id=\"consume-과정\" style=\"position:relative;\"><a href=\"#consume-%EA%B3%BC%EC%A0%95\" aria-label=\"consume 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Consume 과정</h4>\n<p><a href=\"https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/avro_consumer.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">이 예시코드</a>를 살펴보시면 이해가 빠릅니다!</p>\n<ol>\n<li><code class=\"language-text\">AvroDeserializer</code> 객체를 통해 Schema Registry 정보를 받고 직렬화/역직렬화 처리준비를 완료함</li>\n<li>카프카 정보를 받은 후 토픽으로부터 <code class=\"language-text\">subscribe</code> 한다.</li>\n<li>값이 있으면 그때부터 계속 poll하면서 가져온다.</li>\n</ol>\n<h3 id=\"스키마-레지스트리의-작동-원리\" style=\"position:relative;\"><a href=\"#%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%A0%88%EC%A7%80%EC%8A%A4%ED%8A%B8%EB%A6%AC%EC%9D%98-%EC%9E%91%EB%8F%99-%EC%9B%90%EB%A6%AC\" aria-label=\"스키마 레지스트리의 작동 원리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>스키마 레지스트리의 작동 원리</h3>\n<p>스키마 레지스트리 호출을 언급했는데, 정확히 어떤식으로 작동하는 걸까요? 그 내용에 대해선 아래 도식을 통해 설명하겠습니다.</p>\n<ol>\n<li><code class=\"language-text\">Producer</code>는 Avro포맷으로 REST API에 <code class=\"language-text\">POST</code> 를 수행합니다.</li>\n<li>REST interface는 Schama Registry에 스키마를 전송합니다.\n<ol>\n<li>그 후 카프카에 binary data를 전송합니다.</li>\n<li>이때 binary data 에는 schema ID가 포함되어 있습니다.</li>\n</ol>\n</li>\n<li><code class=\"language-text\">Consumer</code>는 binary data를 Kafka에서 consume합니다.\n<ol>\n<li>그 후 해당 정보를 가지고 스키마 레지스트리(혹은 로컬캐시)에서 스키마 정보를 탐색후 가져오고, 이를 이용하여 역직렬화합니다.</li>\n</ol>\n</li>\n</ol>\n<p><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&#x26;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fbj33QF%2FbtqxIoJpMYx%2FicoZHVOOwxU2MHH2xNva2k%2Fimg.png\" alt=\"스키마 레지스트리의 작동방식\"></p>\n<h3 id=\"스키마-레지스트리-서버세팅-docker-compose-사용-시\" style=\"position:relative;\"><a href=\"#%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%A0%88%EC%A7%80%EC%8A%A4%ED%8A%B8%EB%A6%AC-%EC%84%9C%EB%B2%84%EC%84%B8%ED%8C%85-docker-compose-%EC%82%AC%EC%9A%A9-%EC%8B%9C\" aria-label=\"스키마 레지스트리 서버세팅 docker compose 사용 시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>스키마 레지스트리 서버세팅 (Docker Compose 사용 시)</h3>\n<ul>\n<li>\n<p>docker-compose에 스키마 레지스트리 관련 컨테이너 설정을 추가하면 됩니다.</p>\n</li>\n<li>\n<p>docker-compose 내의 환경설정값은 <code class=\"language-text\">'SCHEMA_REGISTRY_'</code> 형태의 prefix를 붙여야합니다.</p>\n</li>\n<li>\n<p>이 설정값은 기본적으로 체크해야 합니다.</p>\n<ul>\n<li><code class=\"language-text\">kafkastore.bootstrap.servers</code>: 연결할 카프카 브로커들의 리스트</li>\n<li><code class=\"language-text\">listeners</code>: HTTP(S)를 통해 API request를 받을 리스너의 리스트</li>\n<li><code class=\"language-text\">kafkastore.connection.url</code>: ZooKeeper URL</li>\n<li><code class=\"language-text\">host.name</code>: 스키마 레지스트리가 여러 노드로 작동중일 때의 설정</li>\n<li><a href=\"https://docs.confluent.io/current/schema-registry/installation/deployment.html#important-configuration-options\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">참고 링크</a></li>\n</ul>\n</li>\n</ul>\n<p>추가로 확인해야할 설정값(SSL, SASL 등의 설정이 필요하다면 아래 링크를 참조해 주세요)</p>\n<ul>\n<li><a href=\"https://docs.confluent.io/current/schema-registry/installation/config.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">링크</a></li>\n</ul>\n<p>deployment시 점검해야할 스펙은 아래와 같습니다:</p>\n<ul>\n<li><a href=\"https://docs.confluent.io/current/schema-registry/installation/deployment.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">링크</a></li>\n</ul>\n<p>서버 세팅이 완료되면, 그 후 스키마 레지스트리 서버에 대해 API 요청을 테스트해볼 수 있습니다:</p>\n<ul>\n<li>schema 등록</li>\n<li>schema 조회</li>\n<li><a href=\"https://docs.confluent.io/current/schema-registry/develop/api.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">링크</a></li>\n</ul>\n<h3 id=\"스키마-레지스트리의-장단점\" style=\"position:relative;\"><a href=\"#%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%A0%88%EC%A7%80%EC%8A%A4%ED%8A%B8%EB%A6%AC%EC%9D%98-%EC%9E%A5%EB%8B%A8%EC%A0%90\" aria-label=\"스키마 레지스트리의 장단점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>스키마 레지스트리의 장단점</h3>\n<p><strong>schema를 사용할 때 생기는 장점</strong></p>\n<ul>\n<li>주고받는 메시지의 스키마 관리가 용이합니다.</li>\n<li>Schema 호환성만 유지된다면, <em>특정 토픽에 여러 버전의 스키마 데이터</em> 를 Produce/Consume이 가능합니다.</li>\n<li>반복되는 값이 많다면 압축률이 상승됩니다(약간의 CPU 연산이 소모됩니다).</li>\n</ul>\n<p><strong>schema를 사용할 때 생기는 단점</strong></p>\n<ul>\n<li>초기 도입이 다소 까다롭습니다.\n<ul>\n<li>Avro Schema를 먼저 정의하고 스키마 레지스트리에 등록 후 그 대로 데이터를 보는데 시간이 어느정도 걸린다.</li>\n<li>JSON 스키마나 Protobuf를 통한 스키마 정의도 마찬가지입니다.</li>\n</ul>\n</li>\n<li>스키마 레지스트리 의 역할이 굉장히 중요; 스키마 레지스트리 의 <strong>장애</strong>가 발생하는 경우 정상적으로 메시지를 <strong>전달하지 못하게</strong> 됩니다.\n<ul>\n<li>Kafka 만 운영하였을때와 비교했을 때, 운영포인트가 증가한다는 것을 의미합니다.</li>\n<li>그렇기 때문에 스키마 레지스트리도 분산운영을 할 수 있는 방안이 있는지 찾아봐야 합니다.\n<img src=\"https://docs.confluent.io/current/_images/multi-dc-setup-kafka.png\" alt=\"img\"></li>\n</ul>\n</li>\n</ul>\n<p>스키마 레지스트리를 강요하지하지는 않지만, 많은 케이스 에서 <a href=\"https://www.confluent.io/blog/schema-registry-kafka-stream-processing-yes-virginia-you-really-need-one/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">스키마 레지스트리 형식으로 포맷을 보장하기를 바라는 것으로 보입니다</a>.</p>\n<h1 id=\"마무리\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마무리</h1>\n<p>이번 글을 통해, 아래 내용들을 살펴볼 수 있었습니다:</p>\n<ol>\n<li>아파치 카프카의 Produce/Consume 뿐 아니라 Streams, Connect 기능을 이용하여 데이터의 흐름을 추가적으로 제어할 수 있음을 알 수 있었습니다.</li>\n<li>데이터 흐름이 복잡해짐에 따라 데이터 직렬화/역직렬화에 대한 관리도구 및 직렬화/역직렬화 도구가 대두되었습니다. 이는 각각 스키마 레지스트리 및 Apache Avro를 말합니다.</li>\n<li>스키마 레지스트리와 Apache Avro에 대해 간략하게나마 살펴보고, 장단점을 살펴보았습니다.</li>\n</ol>\n<p>다음 파트에선 ksqlDB를 명확히 이해하기 위해 Kafka Streams에 대해 명확히 알아보도록 합시다.</p>\n<p>읽어주셔서 대단히 감사합니다.</p>\n<hr>\n<ul>\n<li>References\n<ul>\n<li><a href=\"https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Confluent Schema Registry Tutorial</a></li>\n<li><a href=\"https://dol9.tistory.com/274\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kafka 스키마 관리, Schema Registry</a></li>\n</ul>\n</li>\n</ul>","fields":{"slug":"/devlog/data/2020-02-05---ksqldb-101-pt01//devlog/data/2020-02-05-ksqldb-101-pt01","tagSlugs":["/tag/data-processing/"]},"frontmatter":{"date":"2020-02-05T05:14:00.000Z","description":"ksqlDB를 어느정도 이해하기 위해 필요한 기본지식을 설명하였습니다.","tags":["data_processing"],"title":"ksqlDB 101, part 1. 배경지식","socialImage":{"publicURL":{"publicURL":"/static/06a6ac3882d79b0e2efd58e03fb2c3d0/j1.jpg"}}}}},"pageContext":{"slug":"/devlog/data/2020-02-05---ksqldb-101-pt01//devlog/data/2020-02-05-ksqldb-101-pt01"}},"staticQueryHashes":["251939775","357378587","401334301"]}