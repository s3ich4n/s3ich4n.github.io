---
layout: post
comments: true
title: "Docker pt.02 도커를 자세히 알아보자"
categories: devlog
tags: [infra]
---

# 도커의 개념에 대해 알아보자

도커의 방식에 따라 개발을 한다고 하면 아래와 같다:

1. 컨테이너 구축: 앱
2. 서비스를 구축: 컨테이너가 어떻게 프로덕션 환경에서 작동하는가를 정의
3. 스택을 구축: 모든 서비스 간 상호작용에 대해 정의

## 새로운 환경을 구축하는 방식

원래는 가상환경을 설치하고 요구사항을 적어줘야했다면 이미지로 파이썬 런타임을 얻어둔다. 그 후 빌드는 기존의 파이썬 이미지를 코드에 추가하여 앱, 의존성, 런타임이 동시에 돌아가도록 한다. 이런류의 가상 이미지는 `Dockerfile`안에 정의된다.

## 컨테이너를 `Dockerfile`과 함께 정의

`Dockerfile`은 컨테이너 안에 뭐가 들어갈지 에 대해 정의한 것이다. 네트워크 인터페이스, 디스크 드라이브처럼 리소스에 접근하는 것은 환경내에 가상화 되어있으며 이 것들은 시스템의 나머지와 격리되어있다. 그렇기 때문에 외부에 나가는 포트를 매핑해줘야하고, 어떤 파일이 환경설정에 복사되는지 구체화해야 한다. 이런식으로 정의 한 후에는 `Dockerfile`에 정의된 앱의 빌드가 어디서든 똑같이 작동한다고 할 수 있다.

## 빌드까지 해보자

여기서부터는 Docker의 공식문서와 똑같다.

1. 도커 __이미지__ 를 만든다(`build`)
2. 도커 이미지에 tag를 붙인다
3. 그 후?
    1. 로컬에서 돌린다
        * 로컬에 도는 __컨테이너__ 를 `docker container ls` 로 확인한다
    2. 배포한다 (이를테면 `push`로)
        * `registry`에 배포한다; 이는 리포지토리의 컬렉션
        * 리포지토리는 이미지의 컬렉션; 코드가 이미 빌드되어있다는 점을 빼고는 github 리포랑 비슷한 느낌
        * docker hub에 docker login을 하고 `push`

# Service는 무슨 개념인가?

앱 로드밸런싱과 앱 스케일 조절도 가능해진다. 분산 앱에서는 여러 부분의 앱을 가지고 "services" 라고 부른다. 예를들어 동영상 공유 사이트가 있다고 하자. 거기엔 앱 데이터를 DB에 저장하는 서비스, 사용자가 업로드 후 비디오 인/디코딩을 수행하는 서비스, 프론트엔드 서비스, 등이 있을 것이다.

"service"는 "" 이다. 서비스는 하나의 이미지에서만 돌아가지만, 이미지가 돌아가는 방식을 기술해둔다. 예를들면 몇번 포트가 써져야하는지, 몇가지 컨테이너 복제본이 돌아서 얼마나 버틸지 등이 대표적이다. 서비스를 "스케일링" 하는 것은 소프트웨어 조각을 돌리는 컨테이너 수를 변경하는 것이며, 이는 프로세스 내의 서비스에 많은 컴퓨팅 리소스를 할당하는 것이다.

이런 것들은 도커에선 굉장히 간단하다. `docker-compose.yml` 파일을 만들고 수정하면 되기 때문이다. 도커 공식문서의 예시를 보고 파악해보자.

```yaml
version: "3"
services:
  # web이라는 이름의 서비스를 돌림
  web:
    image: s3ich4n/get-started:pt02
    deploy:
      # 몇개의 동일한 컨테이너를 굴릴건지?
      replicas: 5 
      resources:
        limits:
          # 컨테이너당 10%의 CPU 싱글코어 시간만큼 돌고 메모리는 50M 쓴다
          cpus: "0.1"
          memory: 50M
        restart_policy:
          # 하나가 문제가 생기면 바로 재시작
          condition: on-failure
    ports:
      - "4000:80" # 4000번을 80번으로 매핑
    networks:
      # 'web'의 컨테이너가 80번 포트를 webnet 이라하는
      # 로드밸런스 네트워크에 공유하도록 세팅함
      - webnet 
networks:
  # webnet 네트워크를 기본값으로 둠
  # 이는 로드밸런스 오버레이 네트워크다
  webnet:
```

## 로드밸런스가 적용된 앱을 굴려보자

`docker stack deploy`를 쓰기 전에 아래 명령어를 먼저 입력하자.

> `docker swarm init`

이걸 안치면 이런 에러가 난다: `this node is not a swarm manager`

그 후 앱 이름을 하나 만들어주면 된다.

```docker stack deploy -c docker-compose.yml getstartedlab```

이런 식으로 돌리면 5개의 컨테이너가 돌거다.

`docker-compose.yml`을 바꾸고 `docker stack deploy`를 다시 실행시키면 된다. 껐다가 재부팅할 필요는 없다.

은근히 나오는 `swarm`의 개념은 아래에서 설명하도록 한다.

# Swarm은 어떤 개념인가?

앱을 클러스터에 올리고 여러 머신에서 구동하는 것도 가능하다. 멀티컨테이너, 멀티머신앱을 일종의 도커로 묶인(`Dockerized`) 클러스터로 쓸 수 있다. 이들은 `Swarm`(이하 스웜)이라고 부른다.

## Swarm 클러스터에 대해

`Swarm`은 도커가 작동중이고, 클러스터로 묶인 머신의 그룹이다. 묶이고 난 후에도 도커 커맨드는 그대로 쓸 수 있다. 그렇지만 `swarm manager`에 의해 클러스터 상에서 수행된다. 스웜에 포함된 머신은 물리적일수도, 가상일 수도 있다. 스웜에 포함된 후에는 `node`라고 부른다.

스웜 매니저는 여러 정책을 사용해서 컨테이너를 돌릴 수 있다. 크게 두가지 모드가 있는데, `emptiest node`와 `global`이 그것이다. `emptiest node`는 가장 적게 쓰이는 머신과 컨테이너를 채우는 방식이다. `global`은 각각의 머신들이 정확히 특정한 컨테이너 하나를 가진다고 가정하는 방식이다. 운영시에는 스웜 매니저가 Compose 파일에 이런 정책을 사용하도록 지시한다. 하나의 컨테이너에 대해 하던 것과 비슷하다.

스웜 매니저는 명령어를 수행하거나 다런 머신이 swarm에 `worker`로 들어갈 수 있는 권한을 주는 스웜의 유일한 머신이다. `worker`는 단순히 capacity만을 제공하고 다른기계에게 무엇은 할 수 있고, 무엇은 할 수 없고 를 말할 권한이 없다.

지금까지의 예제는 로컬에서 single-host모드로 도커를 사용했지만, 도커는 swarm mode로 스왑할 수 있다. 이 모드가 스웜을 사용할 수 있게 해준다. 스웜모드를 바로 켜면 현재 머신을 스웜매니저로 만든다. 그 때부터 도커는 본인이 관리하는 스웜에 입력한 명령을 수행한다. 현재 머신이 일을 수행하지 않음에 유의하라.

## 스웜 세팅하기

스웜은 앞서 말했다시피 여러개의 물리/가상노드로 구성되어있다. 스웜을 세팅하는 기본 개념은 다음과 같다. `docker swarm init`을 수행해서 스웜 모드에 돌입해서 현재 머신을 스웜 매니저로 만든다. 그후 다른 머신이 `docker swarm join`을 입력하여 스웜에 worker로 들어오게 한다. 클러스터를 로컬 VM같은걸로 해서 실습하는 세션을 따라해보자.

1. `docker-machine`을 통해 여러개의 가상머신을 구축
2. VM리스트를 보고 IP 확인하기
3. swarm 초기화 및 node 추가
    1. `docker-machine ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>"` 을 수행하면 `myvm1` 노드는 스웜 매니저가 된다.

    ```shell
    $ docker-machine ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>"
    Swarm initialized: current node <node ID> is now a manager.

    To add a worker to this swarm, run the following command:

      docker swarm join \
      --token <token> \
      <myvm ip>:<port>

    To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
    ```


    > Tip: 2377, 2376포트?
    > 
    > 해당 포트는 도커의 스웜 관리용 포트로 쓰인다. 가급적 해당 포트를 피해서 사용하기를 권장한다.

    2. `docker-machine ssh myvm2 "docker swarm join --token <token> <ip>:2377"`를 수행하면 해당 노드는 worker가 된다.

    > Tip: 스웜을 나가려면?
    >
    > `docker swarm leave`를 각 노드별로 실행하기.

4. 앱을 swarm cluster 상에서 구동하기:\
  **스웜 매니저**가 도커 명령을 수행할 수 있다는 점을 염두에 두고 작업하자!
    1. 스웜 매니저의 명령을 바로 수행하기 위해 옵션을 가져오자

    ```shell
    $ docker-machine env myvm1
    export DOCKER_TLS_VERIFY="1"
    export DOCKER_HOST="tcp://192.168.99.100:2376"
    export DOCKER_CERT_PATH="/Users/sam/.docker/machine/machines/myvm1"
    export DOCKER_MACHINE_NAME="myvm1"
    # Run this command to configure your shell:
    # eval $(docker-machine env myvm1)
    ```

    2. 그 후 `myvm1`에서 해당 명령어를 실행시켜 앱을 배포하자!\
    \
    `docker stack deploy -c docker-compose.yml getstartedlab`

    > Docker Hub의 private registry에 있다면 docker login을 먼저 하고\
    > 스웜노드들이 그걸 알고있어야한다!
    >
    > 따라서 먼저 로그인을 하고 `--with-registry-auth -c` 옵션을 함께 주면 된다.
    >
    >```shell
    >$ docker login registry.example.com
    >...
    >$ docker stack deploy --with-registry-auth -c docker-compose.yml getstartedlab
    >...
    >```
    >
    > 이런식으로 로컬에서 스웜 노드들에게 토큰을 보내주어야 정상접근이 가능하다.


설정이 끝나고 `192.168.99.100`, `192.168.99.101` 둘다 접근하면 앱이 뜬다.\
로컬에서 연습할 때는 도커 켜고 실행하는거 까먹지 말고 돌리기!

5. stack 및 swarm을 끄려면?

> `docker stack rm getstartedlab`

> Tip: `docker swarm leave`를 수행하면\
> 현재 속한 스웜에서 나간다. (매니저일 때는 `--force`옵션 필요!)

6. `docker-machine`쉘 환경변수 해지하기

> `eval $(docker-machine env -u)`

# Stack은 어떤 개념인가?

분산앱의 가장 끝자락에 있는 `Stack`이란 개념은, 의존성을 공유하는 상호연관된 서비스들의 그룹이다. 또한 함께 orchestrate되고 스케일을 조절받을 수 있다. 단일 스택은 전체 응용 프로그램의 기능을 정의하고 조정할 수 있다(매우 복잡한 응용 프로그램이 여러 스택을 사용할 수도 있음). 여러 서비스를 같이 한다고 했을 때를 생각해보자! `docker-compose.yml`을 수정하고 예제를 따라해보며 이해해보자. indent에 주의!

1. `docker-compose.yml` 수정하기 (1) \
`visualiser` 추가하기

  ```yaml
  # yml은 indent에 굉장히 민감하기 때문에 더 꼼꼼하게 봐야함

  version: "3"

  services:
    # web이라는 이름의 서비스를 돌림
    web:
      image: s3ich4n/get-started:pt2
      deploy:
        # 몇개의 동일한 컨테이너를 굴릴건지?
        replicas: 3 
        resources:
          limits:
            # 컨테이너당 10%의 CPU 싱글코어 시간만큼 돌고 메모리는 50M 쓴다
            cpus: "0.1"
            memory: "50M"
        restart_policy:
            # 하나가 문제가 생기면 바로 재시작
          condition: on-failure
      ports:
        - "4000:80" # 4000번을 80번으로 매핑
      networks:
        # 'web'의 컨테이너가 80번 포트를 webnet 이라하는
        # 로드밸런스 네트워크에 공유하도록 세팅함
        - webnet
    visualizer:
      image: dockersamples/visualizer:stable
      ports:
        - "8080:8080"
      volumes:
        - "/var/run/docker.sock:/var/run/docker.sock"
      deploy:
        placement:
          constraints: [node.role == manager]
      networks:
        - webnet
  networks:
    # webnet 네트워크를 기본값으로 둠
    # 이는 로드밸런스 오버레이 네트워크다
    webnet:
  ```

  visualiser라는 서비스가 하나 더 추가됐고, 그 속에 `volumes`라는 키와 `placement`라는 키가 함께 추가되었다. `volumes`는 `visualiser`가 호스트의 도커를 위한 소켓파일에 접근권한을 주는 것이고, `placement`는 해당 서비스가 오직 스웜 매니저에서만 작동하도록 하는 것이다(워커는 안됨).
  이 키를 준 이유는 [visualiser가 그렇게 만들어져셔](https://github.com/dockersamples/docker-swarm-visualizer)다.

2. `myvm1` 환경변수를 로드 후 쉘 조작

  위 그림과 같이 `visualiser`는 하나의 단일 이미지를 갖고있고, `web` 인스턴스는 스웜에 나누어져있다. `docker stack ps getstartedlab`으로도 확인할 수 있다. `visualiser`는 스택이 포함된 모든 앱에서 실행할 수 있는 스탠드얼론 서비스다. 다른 것에 의존하지 않는다. 이번엔 **의존**하는 서비스를 달아보자. `Redis`를 통해 방문자 카운트를 수행하는 서비스이다.

3. `docker-compose.yml` 수정하기 (2)\
`redis` 추가하기

```yaml
# yml은 indent에 굉장히 민감하기 때문에 더 꼼꼼하게 봐야함

version: "3"

services:
  # web이라는 이름의 서비스를 돌림
  web:
    image: s3ich4n/get-started:pt2
    deploy:
      # 몇개의 동일한 컨테이너를 굴릴건지?
      replicas: 5
      resources:
        limits:
          # 컨테이너당 10%의 CPU 싱글코어 시간만큼 돌고 메모리는 50M 쓴다
          cpus: "0.1"
          memory: "50M"
      restart_policy:
          # 하나가 문제가 생기면 바로 재시작
        condition: on-failure
    ports:
      - "4000:80" # 4000번을 80번으로 매핑
    networks:
      # 'web'의 컨테이너가 80번 포트를 webnet 이라하는
      # 로드밸런스 네트워크에 공유하도록 세팅함
      - webnet
  visualizer:
    image: dockersamples/visualizer:stable
    ports:
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints: [node.role == manager]
    networks:
      - webnet
  redis:
    image: redis
    ports:
      - "6379:6379"
    volumes:
      - "/home/docker/data:/data"
    deploy:
      placement:
        constraints: [node.role == manager]
    command: redis-server --appendonly yes
    networks:
      - webnet
networks:
  # webnet 네트워크를 기본값으로 둠
  # 이는 로드밸런스 오버레이 네트워크다
  webnet:
```

`Redis`는 도커 라이브러리의 공식이미지가 있고, 그렇기에 이름이 저렇게 짧아도 된다. Redis 포트 6379는 컨테이너에서 호스트로 노출되도록 Redis에 의해 사전 구성되어 있다. 여기에서 Compose 파일에는 호스트에서 리얼월드로 노출되므로 실제로 호스트의 IP 주소를 입력 할 수 있다. 필요에 따라 노드를 Redis Desktop Manager로 가져 와서 Redis 인스턴스를 관리할 수도 있다.

또한 레디스 스펙상에는 몇가지 중요한 사항이 있다. 이것은 구현체들 사이에서 데이터가 변하지 않고 해준다. 다시말해 여러 구현체들이 값을 변경해도 일정하게 유지하도록 해준다는 말이다.

* `redis`는 항상 매니저상에서 돌아간다. 그러므로 동일한 파일 시스템을 사용한다.
* `redis`는 컨테이너 속의 `/data`라는 호스트의 가상 디렉토리를 접근한다. 이는 `Redis`가 데이터를 저장하는 곳이다.

이들이 같이 있음으로서 Redis 데이터에 대한 호스트의 물리 파일시스템에 '단일정보(source of truth)'를 만든다. 이 설정이 없다면, `Redis`는 컨테이너 파일 시스템 내의 /data에 그 값을 저장할 것이며, 컨테이너가 재배포되면 값이 사라질 것이다.

이 '단일정보'는 두가지 구성요소를 가지고 있다.

* Redis 서비스에 둔 제약조건. 이는 항상 동일한 호스트를 가정한다.
* 컨테이너가 호스트의 `./data`를 레디스 컨테이너의 `/data`처럼 접근하도록 한 볼륨. 컨테이너가 동작하는 동안, 특정 호스트의 `./data`에 저장된 파일들이 계속 유지되어 연속성을 유지한다.

4. `./data` 디렉토리를 스웜 매니저에 생성

    `docker-machine ssh myvm1 "mkdir ./data"`

5. `docker stack deploy`를 통해 서비스를 추가

    `$ docker stack deploy -c docker-compose.yml getstartedlab`

# 나만의 앱을 deploy하려면?

크게 두가지가 필요하다.

* `Dockerfile`로 앱 개발
* `docker-compose.yml`로 `Docker app`, `service`, `stack` 환경설정

이 과정을 마치면 `AWS`, `Azure`, 등등 클라우드나 서버에 올리면 될 것이다. 앱이 필요한 프로토콜, 포트를 개방해주는 것도 필요할 것이니 개발시 스스로 알아보고 작업하면 될 것이다. Docker 이미지를 빌드하고 publish하면 어디서든 끌어쓸 수 있을 것이고... 나머지는 검색으로 다시 공부하면 될 것이다.


## References

* [Get Started pt.02 ~ pt.06](https://docs.docker.com/get-started/)
* [초보를 위한 도커 안내서](https://subicura.com/2017/02/10/docker-guide-for-beginners-create-image-and-deploy.html)